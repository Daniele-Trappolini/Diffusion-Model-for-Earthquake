{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import random\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from obspy.imaging.spectrogram import spectrogram\n",
    "from torchmetrics import ScaleInvariantSignalDistortionRatio\n",
    "from torchmetrics import SignalNoiseRatio\n",
    "from torchmetrics.audio import SignalDistortionRatio\n",
    "import torch.nn.functional as F\n",
    "import scipy\n",
    "import seisbench.models as sbm\n",
    "from obspy import Stream,Trace\n",
    "import argparse\n",
    "\n",
    "import Utils.utils_diff as u\n",
    "import Utils.utils_models as um"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Args Parser ###\n",
    "def read_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--dataset_path\",\n",
    "                        default='c:\\\\Users\\\\dantr\\\\Desktop\\\\Github\\\\Dataset\\\\Train\\\\')\n",
    "    \n",
    "    parser.add_argument(\"--checkpoint_path\",\n",
    "                        default='C:\\\\Users\\\\dantr\\\\Desktop\\\\Github\\\\source\\\\Test\\\\Checkpoint\\\\')\n",
    "    \n",
    "    parser.add_argument(\"--ch\",\n",
    "                        default=0,\n",
    "                        type=int,\n",
    "                        help=\"number of channel\")\n",
    "    \n",
    "    parser.add_argument(\"--TRACE_SIZE\",\n",
    "                        default=2496,\n",
    "                        type=int,\n",
    "                        help=\"trace size (default: 3000)\")\n",
    "    \n",
    "        \n",
    "    parser.add_argument(\"--T\",\n",
    "                        default=300,\n",
    "                        type=int,\n",
    "                        help=\"Timesteps (default: 300)\")\n",
    "    \n",
    "    parser.add_argument(\"--batch_size\",\n",
    "                        default=16,\n",
    "                        type=int,\n",
    "                        help=\"Batch size (default: 16)\")\n",
    "\n",
    "    parser.add_argument(\"--signal_start\",\n",
    "                        default=700,\n",
    "                        type=int,\n",
    "                        help=\"signal_start (default: 700)\")\n",
    "    \n",
    "    parser.add_argument(\"--lr\",\n",
    "                        default=0.0001,\n",
    "                        type=float,\n",
    "                        help=\"learning rate (default: 0.0001)\")\n",
    "    \n",
    "    parser.add_argument(\"--train_percentage\",\n",
    "                        default=0.90,\n",
    "                        type=float,\n",
    "                        help=\"train_percentage (default: 0.95)\")\n",
    "    \n",
    "    parser.add_argument(\"--val_percentage\",\n",
    "                        default=0.05,\n",
    "                        type=float,\n",
    "                        help=\"val_percentage (default: 0.025)\")\n",
    "    \n",
    "    parser.add_argument(\"--test_percentage\",\n",
    "                        default=0.05,\n",
    "                        type=float,\n",
    "                        help=\"test_percentage (default: 0.025)\")\n",
    "    \n",
    "    parser.add_argument(\"--seed\",\n",
    "                        default=1234,\n",
    "                        type=int,\n",
    "                        help=\"seed (default: 1234)\")\n",
    "    \n",
    "    parser.add_argument(\"--epochs\",\n",
    "                        default=200,\n",
    "                        type=int,\n",
    "                        help=\"epochs (default: 200)\")\n",
    "    \n",
    "    # args = parser.parse_args()\n",
    "    args, unknown = parser.parse_known_args() # Questo solo per jupyter notebook \n",
    "    \n",
    "    return args\n",
    "\n",
    "args = read_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Questo dovrebbe essere il path poi su github ./Diffusion-Model-for-Earthquake/Stead_Laura/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "path:  c:\\Users\\dantr\\Desktop\\Github\\source\\Train\\\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda\" \n",
    "    map_location=None\n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "    map_location='cpu'\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting seeds\n",
      "len(df noise_train) 500 len(df train) 500\n"
     ]
    }
   ],
   "source": [
    "u.seed_everything(args.seed)\n",
    "force_traces_in_test=[]\n",
    "num_classes=2\n",
    "\n",
    "df = pd.read_pickle(args.dataset_path+\"df_train.csv\")\n",
    "df = df[:500]\n",
    "df=df.drop(columns=[\"level_0\"])\n",
    "df_noise = pd.read_pickle(args.dataset_path+\"df_noise_train.csv\")\n",
    "df_noise = df_noise[:500]\n",
    "print(\"len(df noise_train)\",len(df_noise),\"len(df train)\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events in train dataset:  449\n",
      "Events in validation dataset:  25\n",
      "Events in test dataset:  26\n",
      "Computing E channel\n",
      "Computing N channel\n",
      "Computing Z channel\n",
      "Computing index\n",
      "dataset_trainindex.shape[1] 1\n",
      "X_train.shape:  (449, 2496, 3)\n",
      "index_train.shape:  (449, 1)\n",
      "X_val.shape:  (25, 2496, 3)\n",
      "index_val.shape:  (25, 1)\n",
      "X_test.shape:  (26, 2496, 3)\n",
      "index_test.shape:  (26, 1)\n",
      "data samples in tr_dl:  449\n"
     ]
    }
   ],
   "source": [
    "df, X_train, index_train, X_val, index_val, X_test, index_test= u.train_val_test_split(df, signal_start=args.signal_start, signal_end=args.signal_start+args.TRACE_SIZE, train_percentage=args.train_percentage, val_percentage=args.val_percentage, test_percentage=args.test_percentage,force_in_test=force_traces_in_test)\n",
    "tr_dl = u.create_dataloader(X=X_train, y=X_train, index=index_train,target_dataset=\"train_dataset\", batch_size=args.batch_size,normalize_data=True)\n",
    "val_dl = u.create_dataloader(X=X_val, y=X_val, index=index_val,target_dataset=\"val_dataset\", batch_size=args.batch_size,normalize_data=True)\n",
    "test_dl = u.create_dataloader(X=X_test, y=X_test, index=index_test,target_dataset=\"test_dataset\", batch_size=args.batch_size,normalize_data=True)\n",
    "print(\"data samples in tr_dl: \", len(tr_dl.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events in train dataset:  450\n",
      "Events in validation dataset:  25\n",
      "Events in test dataset:  25\n",
      "Computing E channel\n",
      "Computing N channel\n",
      "Computing Z channel\n",
      "Computing index\n",
      "dataset_trainindex.shape[1] 1\n",
      "X_train.shape:  (450, 2496, 3)\n",
      "index_train.shape:  (450, 1)\n",
      "X_val.shape:  (25, 2496, 3)\n",
      "index_val.shape:  (25, 1)\n",
      "X_test.shape:  (25, 2496, 3)\n",
      "index_test.shape:  (25, 1)\n",
      "data samples in tr_dl:  450\n"
     ]
    }
   ],
   "source": [
    "df_noise, X_train_noise, index_train_noise, X_val_noise, index_val_noise, X_test_noise, index_test_noise=u.train_val_test_split(df_noise, signal_start=args.signal_start, signal_end=args.signal_start+args.TRACE_SIZE, train_percentage=args.train_percentage, val_percentage=args.val_percentage, test_percentage=args.test_percentage,force_in_test=[])\n",
    "tr_dl_noise = u.create_dataloader(X=X_train_noise, y=X_train_noise, index=index_train_noise,target_dataset=\"train_dataset\", batch_size=args.batch_size)\n",
    "val_dl_noise = u.create_dataloader(X=X_val_noise, y=X_val_noise, index=index_val_noise,target_dataset=\"val_dataset\", batch_size=args.batch_size)\n",
    "test_dl_noise = u.create_dataloader(X=X_test_noise, y=X_test_noise, index=index_test_noise,target_dataset=\"test_dataset\", batch_size=args.batch_size)\n",
    "print(\"data samples in tr_dl: \", len(tr_dl_noise.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define beta schedule\n",
    "betas = u.linear_beta_schedule(timesteps=args.T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e784db9f623a4b46b637d083aa9be55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train 0.7296901941299438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7d171c6a8b45eab7e1b1c819942edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_val_loss 0.7442770600318909  min_loss inf\n",
      "Best Epoch: 1\n",
      "model saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd79cf8a88404b77bc214a01461fb235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train 0.7405068278312683\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cb7572c7c44d6f9819192454ec0632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_val_loss 0.7213833332061768  min_loss 0.7442770600318909\n",
      "Best Epoch: 2\n",
      "model saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eed2d6bafef4827b8648cc090af4d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train 0.717646598815918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30633107bb7431394c7c73453ec5504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_val_loss 0.707526445388794  min_loss 0.7213833332061768\n",
      "Best Epoch: 3\n",
      "model saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030dd1d674f240f08c8c235a18900137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train 0.70143061876297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b12723479544569d845bc20d8dec72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_val_loss 0.7031758427619934  min_loss 0.707526445388794\n",
      "Best Epoch: 4\n",
      "model saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29a4fd219684be89166f51a9010f277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train 0.7003443241119385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1792688870cd4884bbca952bc785fa38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_val_loss 0.6947039365768433  min_loss 0.7031758427619934\n",
      "Best Epoch: 5\n",
      "model saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260de865db61426d8ab386b797e25db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1412\\2674506849.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mreduce_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m65\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mnoise_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnoise_in\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mreduce_noise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mf_signal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_signal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_noisy_signal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_eq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_noisy_signal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0meq_mask_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dantr\\Desktop\\Github\\source\\Train\\Utils\\utils_models.py\u001b[0m in \u001b[0;36mcreate_mask\u001b[1;34m(noise_in, eq_in)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0mf_signal_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_signal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_signal_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_signal_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_noisy_signal_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_eq_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_noise_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = sbm.DeepDenoiser()\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "min_loss = np.Inf\n",
    "max_si_sdr = -np.Inf\n",
    "si_sdr = ScaleInvariantSignalDistortionRatio()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    model.train() \n",
    "    for step, (batch, noise_in) in tqdm(enumerate(zip(tr_dl, tr_dl_noise)),total = len(tr_dl)):\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        noise_in = noise_in[0].permute(0,2,1).float()[:,args.ch,:].reshape(args.batch_size,1,args.TRACE_SIZE).to(device)\n",
    "        x=batch[0].permute(0,2,1).float()[:,args.ch,:].reshape(args.batch_size,1,args.TRACE_SIZE).to(device)\n",
    "        reduce_noise = random.randint(40, 65)*0.01\n",
    "        noise_in = noise_in*reduce_noise\n",
    "        f_signal, t_signal, tmp_noisy_signal,y_eq, y_noise = um.create_mask(noise_in.cpu(),x.cpu())\n",
    "        out = model(tmp_noisy_signal.to(device))\n",
    "        eq_mask_pred=out[:,0,:]\n",
    "        noise_mask_pred=out[:,1,:]\n",
    "        mask = torch.stack([y_eq,y_noise],axis = 1)\n",
    "        loss=F.cross_entropy(out.to(device),mask.to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"loss train\", loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sum_si_sdr_val = 0\n",
    "        sum_val_loss=0\n",
    "        for step, batch in tqdm(enumerate(val_dl), total=len(val_dl)):\n",
    "            noise_in = next(iter(val_dl_noise))[0].permute(0,2,1).float()[:,args.ch,:].reshape(args.batch_size,1,args.TRACE_SIZE).to(device)#\n",
    "            x=batch[0].permute(0,2,1).float()[:,args.ch,:].reshape(args.batch_size,1,args.TRACE_SIZE).to(device)#\n",
    "            reduce_noise=random.randint(40, 65)*0.01\n",
    "            noise_in=noise_in*reduce_noise\n",
    "            f_signal, t_signal, tmp_noisy_signal,y_eq, y_noise=um.create_mask(noise_in.cpu(),x.cpu())\n",
    "            out = model(tmp_noisy_signal.to(device))\n",
    "            eq_mask_pred=out[:,0,:]\n",
    "            noise_mask_pred=out[:,1,:]\n",
    "            mask = torch.stack([y_eq,y_noise],axis = 1)\n",
    "            sum_val_loss+=F.cross_entropy(out.to(device),mask.to(device)).item()*args.batch_size\n",
    "            curr_val_loss = sum_val_loss/(len(val_dl)*args.batch_size)\n",
    "        print(\"curr_val_loss\",curr_val_loss,\" min_loss\",min_loss) \n",
    "        if curr_val_loss < min_loss:\n",
    "            min_loss = curr_val_loss\n",
    "            torch.save(model.state_dict(), args.checkpoint_path+\"final_epoch\"+str(epoch)+\"DeepDen.pt\")\n",
    "            print(\"Best Epoch:\", epoch+1)\n",
    "\n",
    "    print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
